{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Sequence-to-Sequence CNN-BiLSTM Based Glottal Closure Instant Detection from Raw Speech\n",
    "\n",
    "This is an example of a Python code to train and test a CNN-BiLSTM model, a joint convolutional (CNN) and recurrent (RNN) neural network model, for detecting glottal closure instants (GCIs) in the speech signal. See the [corresponding paper](paper/Matousek_ANNPR2022_paper.pdf) for more details.\n",
    "\n",
    "[Keras](https://keras.io/) (v2.6.0) with [TensorFlow](https://www.tensorflow.org/) (v2.6.0) backend are used to train and evaluate the CNN-BiLSTM model.\n",
    "\n",
    "Prerequisities are stored in the [requirements](requirements.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import random as pyrandom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "import librosa as lr\n",
    "import logging\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the training and evaluation of the CNN-BiLSTM model, we describe data firstly. Note that just a [sample of data](data/sample) will be used in this tutorial (only 40 waveforms for training and 2 waveforms for testing from 2 voice talents). In the [corresponding paper](paper/Matousek_ANNPR2022_paper.pdf), 3200 waveforms from 16 voice talents were used.\n",
    "\n",
    "The following sample of data is used:\n",
    "* `spc ...` speech waveforms sampled at 16 kHz\n",
    "* `gci ...` ground truth GCIs\n",
    "\n",
    "We used the [Multi-Phase Algorithm](http://www.sciencedirect.com/science/article/pii/S0167639311000094) (MPA) to detect GCIs from the contemporaneous electroglottograph (EGG) signal and used the detected GCIs as the ground truth ones.\n",
    "\n",
    "The ground truth GCIs are available in the [wavesurfer](http://www.speech.kth.se/wavesurfer) format\n",
    "\n",
    "```\n",
    "0.234687 0.234687 V\n",
    "0.242312 0.242312 V\n",
    "0.250250 0.250250 V\n",
    "0.258062 0.258062 V\n",
    "0.265937 0.265937 V\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets the randomness and tries to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "SEED = 8\n",
    "# Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# Set python built-in pseudo-random generator at a fixed value\n",
    "pyrandom.seed(SEED)\n",
    "# Set numpy pseudo-random generator at a fixed value\n",
    "np.random.seed(SEED)\n",
    "# Set the tensorflow pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(SEED)\n",
    "# Suppress Tensorflow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up various (hyper-)parameters. Note that for this demonstration we use a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "spc_dir = 'data/spc'         # directory with speech waveforms\n",
    "pm_dir = 'data/gci'          # directory with ground truth GCIs\n",
    "trainlist = 'data/train.txt' # list of training utterances\n",
    "vallist = 'data/val.txt'     # list of validation utterances\n",
    "\n",
    "# Windowing\n",
    "freq_samp = 16000  # sampling frequency of the signal to detect in\n",
    "win_len = 0.010    # window length (sec)\n",
    "hop_len = 0.002    # hop length (sec)\n",
    "window = 'boxcar'  # 'boxcar' is equivalent to rectangular window\n",
    "n_timesteps = 900  # number of time steps\n",
    "pad_value = 0      # value to pad shodrter signals\n",
    "\n",
    "# RNN layers\n",
    "rnn_cells = (64,) # in the paper we use (256, 256, 256)\n",
    "dropout = (0.5,)  # in the paper we use (0.5, 0.5, 0.5)\n",
    "batch_norm = True # whether to use batch normalization\n",
    "rnn = 'lstm'      # units used in the RNN architecture ('lstm' or 'gru')\n",
    "\n",
    "# CNN layers\n",
    "n_convs = 1            # No. of repeating convolutional layers in a single CNN block (typically 1-2) - in the paper we use 2\n",
    "filters = (4,)         # No. of filters corresponds to No. of convolutional blocks - in the paper we use (16, 32, 64)\n",
    "kernel_size = (3,)     # Kernel size for each convolutional block (must be the same as the number of filters)\n",
    "strides = (1,)         # in the paper we use (1, 1, 1)\n",
    "pool_size = (2,)       # in the paper we use (1, 1, 1)\n",
    "padding = ('same',)    # in the paper we use ('same', 'same', 'same')\n",
    "\n",
    "# Training\n",
    "batch_size = 64            # in the paper we use 16\n",
    "n_epochs = 4               # in the paper we use 100\n",
    "optimizer = 'adam'         # optimizer\n",
    "learning_rate = 0.001      # in the paper we use 0.0005\n",
    "early_stop_patience = None # in the paper we use 10\n",
    "\n",
    "# Set up measures for GCI detection\n",
    "abs_dist = 0.00025    # distance threshold (msec) to detect shifted GCIs (used for IDA - identification accuracy error) \n",
    "rel_dist = 10         # distance threshold (integer percentage within current T0) to detect shifted GCIs\n",
    "min_t0 = 0.02         # minimum T0 for relative-distance based comparison\n",
    "sync = (0.002, 0.002) # syncing interval to search for minimum negative sample in the detected speech frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Training the CNN-BiLSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firsly, we load train/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/dev utterance lists\n",
    "trainset = np.loadtxt(trainlist, dtype=str).tolist()\n",
    "valset = np.loadtxt(vallist, dtype=str).tolist()\n",
    "# Window length in samples\n",
    "n_wsamples = lr.time_to_samples(win_len, freq_samp)\n",
    "\n",
    "# Load train data\n",
    "X_train, Y_train, _ = ut.prep4input(trainset, spc_dir, pm_dir, freq_samp, n_timesteps, fft_len=win_len, hop_len=hop_len,\n",
    "                                    window=window, pad_value=pad_value)\n",
    "# Load validation data\n",
    "X_val, Y_val, _ = ut.prep4input(valset, spc_dir, pm_dir, freq_samp, n_timesteps, fft_len=win_len, hop_len=hop_len,\n",
    "                                window=window, pad_value=pad_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and check the shape of inputted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape       : (95, 900, 160, 1) (95, 900, 1)\n",
      "Validation data shape     : (4, 900, 160, 1) (4, 900, 1)\n",
      "# training targets (1/0)  :  85500 (8701 / 76799)\n",
      "# validation targets (1/0): 3600 (377 / 3223)\n",
      "# samples per window      : 160\n"
     ]
    }
   ],
   "source": [
    "print('Training data shape       :', X_train.shape, Y_train.shape)\n",
    "print('Validation data shape     :', X_val.shape, Y_val.shape)\n",
    "print('# training targets (1/0)  :  {} ({} / {})'.format(Y_train.size, len(Y_train[Y_train==1]), len(Y_train[Y_train==0])))\n",
    "print('# validation targets (1/0): {} ({} / {})'.format(Y_val.size, len(Y_val[Y_val==1]), len(Y_val[Y_val==0])))\n",
    "print('# samples per window      :', n_wsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In this example, we use a joint CNN-BiLSTM model which is shown in the paper to achieve the best results. The model is defined by `utils.create_model` function. A simplified scheme of the proposed CNN-BiLSTM based GCI detection is shown here:\n",
    "\n",
    "![GCI detection](figs/gci_detection.png \"CNN-BiLSTM based GCI detection model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN block (dotted line) works as a feature extractor. When omitted, \"only\" RNN-based GCi detection on raw speech is performed. For 16kHz input speech, three BiLSTM layers with 256 cells in each layer and 900 time steps were used in RNN blocks. In CNN blocks, three convolutional blocks with two convolutional layers in each block followed by batch normalization and maximum pooling layers were used (with the number of filters 16, 32, 64, kernel size 7 with stride 1, pooling size 3, and \"same\" padding). The dense layer outputs a prediction whether or not a frame contains a GCI. The dotted speech signals at the top indicate that no GCI was detected in the corresponding speech frames; otherwise, red circles mark GCI location.\n",
    "\n",
    "The model is created and a summary is outputted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 160, 1)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 160, 4)      16        \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 160, 4)      16        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, 160, 4)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 80, 4)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 320)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         197120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 197,793\n",
      "Trainable params: 197,529\n",
      "Non-trainable params: 264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ut.create_model((None, n_wsamples, 1), len(filters), n_convs, filters, kernel_size, strides, padding, pool_size,\n",
    "                        rnn_cells, dropout, batch_norm, rnn)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {'adam': Adam(learning_rate=learning_rate)}[optimizer]\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can train the model on the train set and evaluate it on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2/2 [==============================] - 8s 2s/step - loss: 1.0083 - accuracy: 0.4762 - val_loss: 0.6855 - val_accuracy: 0.7481\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 4s 1s/step - loss: 0.7870 - accuracy: 0.7107 - val_loss: 0.6675 - val_accuracy: 0.8958\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 4s 1s/step - loss: 0.6890 - accuracy: 0.6517 - val_loss: 0.6543 - val_accuracy: 0.8983\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.6400 - accuracy: 0.6793 - val_loss: 0.6442 - val_accuracy: 0.9025\n"
     ]
    }
   ],
   "source": [
    "history, stopped_epoch, best_epoch_idx = ut.fit_model(model, X_train, Y_train, validation_data=(X_val, Y_val),\n",
    "                                                      epochs=n_epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In this very simplified example, the accuracy on the validation set was about 90% (it could differ due to the randomness). Much better results can be obtained when more training data from more voice talents is used, when tuning the hyper-parameters (such as the batch size and learning rate) and the complexity of the model (the number of CNN and/or RNN layers, the number of LSTMS cells etc.) is done and also when the model is trained for more epochs. Please see the [paper](paper/Matousek_ANNPR2022_paper.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the CNN-BiLSTM model\n",
    "\n",
    "GCI detection techniques are usually evaluated by comparing locations of the detected and ground truth GCIs in terms of identification rate (IDR), miss rate (MR), false alarm rate (FAR), identification rate (IDA), accuracy within 0.25 ms range (A25), and we also use a dynamic evaluation measure (E10) (please see the [paper](paper/Matousek_ANNPR2022_paper.pdf) for more details.). \n",
    "\n",
    "We can evaluate our simple model and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IDR': 0.14392059553349876,\n",
       " 'MR': 0.8461538461538461,\n",
       " 'FAR': 0.009925558312655087,\n",
       " 'IDA': 0.0006717221939355704,\n",
       " 'A25': 0.9516129032258065,\n",
       " 'E10': 0.13647642679900746}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ut.evaluate_model(model, valset, spc_dir, pm_dir, freq_samp, n_timesteps, win_len, hop_len,\n",
    "                            window, pad_value, sync, abs_dist, rel_dist, min_t0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our simple model performes very poorly, with the _identification rate_ (IDR) being only 14.39%. Definitely, a better model is needed. We have tuned the hyper-parameters of the CNN-BiLSTM model and trained it on all data (3200 utterances) using GPU (see the [paper](paper/Matousek_ANNPR2022_paper.pdf)). The resulting pre-trained weigths are available [here](models/CNN-BiLSTM/weights.h5): `models/CNN-BiLSTM/weights.h5` and the model's architecture is [here](models/CNN-BiLSTM/architecture.json): `models/CNN-BiLSTM/architecture.json`. They could be used to set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/CNN-BiLSTM/architecture.json', 'rt') as json_file:\n",
    "    model = model_from_json(json_file.read())\n",
    "# Load optimal model's weights from hdf5 file\n",
    "model.load_weights('models/CNN-BiLSTM/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we achieve much better results (IDR = 97.52%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IDR': 0.9751861042183623,\n",
       " 'MR': 0.009925558312655087,\n",
       " 'FAR': 0.01488833746898263,\n",
       " 'IDA': 0.0002446933776474915,\n",
       " 'A25': 0.9924812030075187,\n",
       " 'E10': 0.967741935483871}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ut.evaluate_model(model, valset, spc_dir, pm_dir, freq_samp, n_timesteps, win_len, hop_len,\n",
    "                            window, pad_value, sync, abs_dist, rel_dist, min_t0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCI detection results for the proposed model (CNN-BiLSTM) and the comparison with other models and algorithms on publicly available [CMU](http://festvox.org/dbs/index.html) datasets are shown in the following table. Again, please see the [paper](paper/Matousek_ANNPR2022_paper.pdf) for more details.\n",
    "\n",
    "![evaluation](figs/evaluation.png \"GCI detection evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting GCIs with the CNN-BiLSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detection, we use unseen speech waveforms. Here we use three utterances from the [CMU](http://festvox.org/dbs/index.html) dataset (voice [BDL](http://festvox.org/cmu_arctic/dbs_bdl.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predlist = ['bdl_arctic_a0001', 'bdl_arctic_a0002', 'bdl_arctic_a0003']\n",
    "pms, _ = ut.detect(model, predlist, 'prediction/spc', freq_samp, n_timesteps, win_len, hop_len, window, pad_value, sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the times of the detected GCIs, e.g. the first 10 GCIs in the first utterance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2346875,\n",
       " 0.2423125,\n",
       " 0.25025,\n",
       " 0.2580625,\n",
       " 0.2659375,\n",
       " 0.273875,\n",
       " 0.2816875,\n",
       " 0.289375,\n",
       " 0.2970625,\n",
       " 0.30475]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.time for p in pms[0][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can store the detected GCIs in the [wavesurfer](http://www.speech.kth.se/wavesurfer) format using the `Pitchmark` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pm, fn in zip(pms, predlist):\n",
    "    # Write GCIs to a file\n",
    "    pm.write_file(osp.join('prediction/gci_pred', fn+'.pm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the example of the detected GCIs:\n",
    "\n",
    "![GCI detection sample](figs/gci_detection_sample.png \"GCI detection sample\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
